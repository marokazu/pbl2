ランダムな森を動かす決定樹
これまで、単純な生成分類子(単純ベイズ;詳細は単純ベイズ分類を参照)と強力な判別分類子(サポートベクトルマシン;詳細:サポートベクトルマシンを参照)について詳しく見てきました。ここでは、別の強力なアルゴリズムであるランダムフォレストと呼ばれるノンパラメトリックアルゴリズムの動機付けについて説明します。ランダム森林はアンサンブル法の一例であり,より単純な推定量のアンサンブルの結果を集約することに依存している。このようなアンサンブル手法を用いた場合のいくぶん驚くべき結果は、和が部分より大きくなることである。すなわち、多数の推定者の多数決が、投票を行っている個々の推定者のいずれよりも優れていることになるのである。この例については、次のセクションで説明します。まず、標準インポートについて説明します。

In[1]

ランダムな森は決定木上に構築されたアンサンブル学習者の例である。このため、まず意思決定ツリー自体について説明します。

意思決定ツリーは、オブジェクトを分類またはラベル付けするための非常に直感的な方法です。分類に取り掛かるための一連の質問をするだけです。たとえば、ハイキング中に遭遇する動物を分類するための決定木を作成する場合は、次のようなものを作成します。

figure[1]

よく構築されたツリーでは、各質問によってオプションの数が約半分に削減され、多数のクラスの中でさえもオプションが非常に速く狭められます。もちろん、ポイントは各ステップでどの質問をするかを決めることです。決定木の機械学習実装では,質問は一般にデータの軸整列分割の形をとる:すなわち,ツリー中の各ノードは,特徴の1つの中でカットオフ値を用いてデータを2つのグループに分割する。この例を見てみましょう。


意思決定ツリーの作成
4つのクラスラベルのいずれかを持つ次の2次元データについて考えます。

In[2]
figure[2]

このデータに基づいて構築された単純な意思決定ツリーは、何らかの定量的基準に従って、一方の軸または他方の軸に沿ってデータを繰り返し分割し、各レベルで、その中の点の多数決に従って、新領域のラベルを割り当てる。この図は、このデータの決定木分類子の最初の4つのレベルを視覚化したものです。

figure[3]

最初の分割後、上のブランチのすべてのポイントは変更されないため、このブランチをさらに分割する必要はありません。1つのカラーすべてを含むノードを除き、各レベルで、すべての領域が再び2つのフィーチャのいずれかに沿って分割されます。

決定木をデータに適合させるこのプロセスは、Scikit-LearnのDecisionTreeClassifier評価プログラムで実行できます。

In[3]

分類子の出力を視覚化するのに役立つ簡単なユーティリティ関数を作成します。

In[4]

次に、意思決定ツリーの分類がどのようなものかを調べます。

In[5]
figure[4]

このノートブックを実際に使用している場合は、オンライン付録に含まれているヘルパースクリプトを使用して、意思決定ツリーの作成プロセスをインタラクティブに視覚化できます。

In[6]
figure[5]

深さが増すと非常に奇妙な形の分類領域ができます;たとえば、深さが5の場合、黄色と青の領域の間に、高くて薄い紫色の領域があります。これが真の固有データ分布の結果ではなく、データの特定のサンプリングまたはノイズ特性の結果であることは明らかです。つまり、この意思決定ツリーは、深さがわずか5レベルであっても、明らかに私たちのデータを過剰に満たしています。

意思決定ツリーと過剰適合
このようなオーバーフィッティングは、ディシジョンツリーの一般的な特性であることが判明します。つまり、ツリーの深さがあまりにも深くなりすぎると、データが引き出される分布の全体的な特性ではなく、特定のデータの詳細に適合しやすくなります。このオーバーフィッティングを確認するもう1つの方法は、データの異なるサブセットでトレーニングされたモデルを確認することです。たとえば、この図では、2つの異なるツリーをそれぞれ元のデータの半分でトレーニングしています。

figure[6]

ある場所では、この二つの系統樹が一貫した結果を示す(例えば、四隅)ことは明らかであるが、他の場所では、この二つの系統樹は非常に異なった分類を示す(例えば、任意の2つのクラスター間の領域)。重要な観察は、分類が不確実な場合に矛盾が生じる傾向があることであり、したがって、これらのツリーの両方からの情報を使用することによって、より良い結果が得られる可能性があります。

このノートブックをライブで実行している場合、次の関数を使用すると、データのランダムなサブセットでトレーニングされた樹木のフィットをインタラクティブに表示できます。

In[7]
figure[7]

2つの木からの情報を使用することで結果が改善されるのと同様に、多くの木からの情報を使用することで結果がさらに改善されると期待できます。

乱伐の森
複数のオーバーフィッティング推定量を組み合わせてこのオーバーフィッティングの影響を低減できるというこの概念は、バッギングと呼ばれるアンサンブル手法の基礎となっている。バッギングでは,並列推定量のアンサンブル(たぶん手提げ袋)を利用し,それぞれがデータに過剰適合し,結果を平均化してより良い分類を見つける。ランダム化決定木の集合はランダムフォレストとして知られている。

このタイプのバッギング分類は、次に示すように、Scikit-LearnのBaggingClassifierメタエスティメータを使用して手動で行うことができます。

In[8]
figure[8]

この例では、各推定器をトレーニングポイントの80%のランダムなサブセットに適合させることによってデータをランダム化した。実際には、ディシジョン・ツリーは、分割の選択方法に確率性を注入することによって、より効果的にランダム化されます。このようにして、すべてのデータが毎回適合に寄与しますが、適合の結果は依然として望ましいランダム性を持っています。たとえば、どのフィーチャを分割するかを決定する場合、ランダム化されたツリーは、上位のいくつかのフィーチャから選択することができます。これらのランダム化戦略に関する技術的な詳細については、のScikit-Learnドキュメントおよびリファレンスを参照してください。

Scikit‐Learnでは,ランダム化された決定木のこのような最適化されたアンサンブルは,すべてのランダム化を自動的に処理するRandomForestClassifier推定器で実装される。必要なのは、いくつかの推定量を選択することだけで、木の集合に非常にすばやく(必要に応じて並行して)適合する。

In[9]
figure[9]

ランダムに摂動を受けた100以上のモデルを平均化することで、パラメータ空間をどのように分割すべきかについての直感に非常に近い全体的なモデルが得られることがわかります。

ランダムフォレスト回帰
前節では、分類の文脈の中でランダムフォレストについて考察した。回帰の場合には、ランダムな森林も機能するようにすることができる(すなわち、カテゴリ変数ではなく連続変数)。これに使用する推定量はRandomForestRegressorで、構文は先に見たものと非常によく似ています。

速い振動と遅い振動の組み合わせから得られた次のデータを考えてみよう。

In[10]
figure[10]

ランダムフォレスト回帰分析を使用すると、次のような最適フィットカーブを見つけることができます。

In[11]
figure[11]